# A neural network approach for supporting gesture keyboards in Augmented Reality
Augmented Reality (AR) can superimpose 3D virtual objects in a 3D real environment to create a mix reality experience that allows the user to simultaneously interact with digital and physical objects. The core value of AR is not a simple display of data, but the integration of immersive sensations and the natural interactions between human and the virtual objects. 
\\Text input is a key interaction method that allows users to enter text such as for sending messages or writing documents. Automatic Speech Recognition (ASR) is one of the methods. However, ASR is infeasible under the noisy environment such as in factories or in crowded and public places. We proposed an AR Gesture Keyboard (ARGK), which is an analogy to the word-gesture keyboard in smart mobile phones. This gesture input method can potentially beat the state-of-the-art AR text entry technique, which is discussed in the Section \textit{Results} based on a simulated gesture recogniser. This simulated recogniser simulates a real recogniser which is a machine learning model that can map the gesture trace to text. 
\\We then carried out 20 user experiments to collect the gesture data, and the data were analysed in depth. The data collection experiment requires not only the HoloLens itself but also a tracking system that tracks the fingertip and the HoloLens so that the relative position and rotation angle between them can be computed. OptiTrack is used as the tracking system since it offers a system with low latency and high accuracy. This analysis can not only enable us to study the user's natural behaviours but also give us insights on the characteristics of natural gesture traces so that we can implement the characteristics into the synthesiser training in the future. Therefore, the project's goal is to capture and analyse the characteristics of the gesture trace, user behaviour and to assess the potential of this ARGK. Furthermore, different metrics that measure the characteristics of the gesture data were proposed and analysed so that the user's behaviour can be explained in a more systematic way. The micro metrics measure the accuracy of the users while they gesture on the keyboard, how fast they can gesture a complete phrase in terms of word per minute (WPM), how the users perceive where the keyboard lies and how consistent they are at gesturing the phrases in terms of the relative position between the plane that their gesture trace lies on and the virtual keyboard plane. 
\\The outcomes, not only the micro metrics but also the collected gesture trace data, of this project were anticipated to allow future work to generate artificial traces. Since this project has completed the primary objectives, we began to investigate the generation of artificial traces. Therefore, the future work is to use deep learning to generate artificial traces and to map the natural and artificial traces to text. We proposed a neural network to generate artificial traces. The neural network consists of two LSTM layers and one attention mechanism. Different optimisers and training techniques were used to optimise the training process and performance. The performance of this synthesiser can be measured not only by classifiers (if the synthesiser has a representative output, then the classifier can not classify artificial and natural gesture traces) but also by the micro metrics proposed in the early stage. 
\\In general, the whole project contains three main stages. Firstly, develop the ARGK with a simulated recogniser, Secondly, run user experiments and obtain the micro metrics quantitatively from the collected data, Thirdly, develop a full ARGK with a machine learning-based recogniser by using a combination of collected data and synthetic data (which is synthesised from the collected data using another neural network). The fourth-year project contains the first and second stage. The second stage is incredibly important for the third stage since not only the trace data is very limited and it can help to generate synthetic data, but also it quantitatively gives the great potential of the full ARGK system. I have finished the second stage and am currently working on the third stage, which is beyond the original project plan.


## ARGK_with_simulated_recogniser
This folder contains the simulated recogniser implemented in the gesture keyboard. 

## Generator
It is the neural network I constructed to generate synthetic gesture traces. 

## Opti and Holoframe
They transform the OptiTrack data and Hololens data to the HMD frame seprately

## Data Analysis 
It contains scripts to compute the micro metrics of the data obtained from the user experiments. It also shows how the data is preprocessed and orgnised. 
